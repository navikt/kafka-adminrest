apiVersion: "nais.io/v1alpha1"
kind: "Alert"
metadata:
  name: kafka-adminrest-alerts
  labels:
    team: integrasjon
spec:
  receivers:
    slack:
      channel: '#integrasjon-alerts'
  alerts:
    - alert: kafka-adminrest-app-nede
      expr: up{app=~"kafka-adminrest",job="kubernetes-pods"} == 0
      for: 2m
      description: "{{ $labels.app }} er nede i {{ $labels.kubernetes_namespace }}"
      action: "Se `kubectl describe pod {{ $labels.kubernetes_pod_name }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }}` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: kafka-adminrest-kontinuerlig-restart
      expr: sum(increase(kube_pod_container_status_restarts_total{container=~"kafka-adminrest"}[30m])) by (container) > 2
      for: 5m
      description: "{{ $labels.container }} har restartet flere ganger siste halvtimen!"
      action: "Se `kubectl describe pod {{ $labels.container }}` for events, og `kubectl logs {{ $labels.container }}` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: kafka-adminrest-mangler-metrikker
      expr: absent(up{app=~"kafka-adminrest",job="kubernetes-pods"})
      for: 5m
      description: "{{ $labels.app }} rapporterer ingen metrikker i {{ $labels.kubernetes_namespace }}"
      action: "Sjekk om {{ $labels.app }} i {{ $labels.kubernetes_namespace }} er oppe"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: kafka-adminrest-errorlogging
      expr: increase(logd_messages_total{log_app=~"kafka-adminrest",log_level="Error"}[2m]) > 0
      for: 0m
      description: "{{ $labels.log_app }} rapporterer error i loggene"
      action: "Sjekk loggene"
      sla: respond within 1h, during office hours
      severity: warning
